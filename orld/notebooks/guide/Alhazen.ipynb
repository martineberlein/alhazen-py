{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc74cab",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction to _Alhazen-Py_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d43d33",
   "metadata": {},
   "source": [
    "The initial idea for writing this guide was to explain to our students at Humboldt-Universität Zu Berlin how to use _Alhazen_ to determine the failure circumstances of a program. The original notebooks were a joint project with my colleague [Hoang Lam Nguyen](https://www.informatik.hu-berlin.de/en/Members/hoang-lam-nguyen) from Humboldt-Universität Zu Berlin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a4934",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Considering the difficulties of determining the circumstances of a program’s behavior, Kampmann et al. [[KHSZ20](https://publications.cispa.saarland/3107/7/fse2020-alhazen.pdf)] presented an approach to automatically learn the associations between the failure of a program and the input data. Their tool _Alhazen_ affiliates specific syntactical input features, like input length or the presence of particular derivation sequences, with the behavior in question. This allows _Alhazen_ to hypothesize why failure-inducing input files result in a defect. In this notebook, we will extend _Alhazen_ [[KHSZ20](https://publications.cispa.saarland/3107/7/fse2020-alhazen.pdf)] and build our own framework to determine and explain the failure of a program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1265b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To run this notebook, you should have the following Python packages installed. Please refer to the README.md of this project to get a complete list of requirements and instructions on how to install them.\n",
    "\n",
    "- pandas, numpy\n",
    "- fuzzingbook\n",
    "- jupyter-notebook\n",
    "- sklearn\n",
    "- graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531c03e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "[Tip]: To execute the Python code in the code cell below, click on the cell to select it and press <kbd>Shift</kbd> + <kbd>Enter</kbd>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ab122",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# To import other notebooks we need this dependency\n",
    "# !pip install ipynb fuzzingbook pandas numpy sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c96ae1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Verbose output\n",
    "# Set to True to see some intermediate results\n",
    "display_output = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e7d85e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Overview of Alhazen-Py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b1eb6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![title](img/overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d1a41b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Recent testing techniques, like fuzzing [[MFS90](https://dl.acm.org/doi/pdf/10.1145/96267.96279), [FMEH20](https://www.usenix.org/system/files/woot20-paper-fioraldi.pdf)], generate random input data and enhance or mutate them to trigger potential defects or software vulnerabilities. Although they have proven capable of detecting and generating erroneous input data, they often lack an explanation of why specific input data results in incorrect behavior. However, when diagnosing why a program fails, the first step is determining the circumstances under which the program failed. Kampmann et al. [[KHSZ20](https://publications.cispa.saarland/3107/7/fse2020-alhazen.pdf)] presented an approach to automatically discover the circumstances of program behavior. Their method associates the program’s failure with the syntactical features of the input data, allowing them to learn and extract the properties that result in the specific behavior. As a result, their tool Alhazen can generate a diagnosis and explain why, for instance, a particular bug occurs. More formally, Alhazen forms a hypothetical model based on the observed inputs. Then, additional test inputs are generated and executed to refine or refute the hypothesis, eventually obtaining a prediction model of why the behavior in question occurs. Alhazen uses a Decision Tree classifier to learn the association between the program behavior and the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b06a4f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This collection of notebooks contain a reduced implementation **alhazen-py** that we will complete together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1cf59a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **To complete the feedback loop of alhazen-py we will look at the following individual tasks:**\n",
    "\n",
    "**Activity 1:** Initial Setup and Feature Extraction \n",
    "\n",
    "2. Task: Write the functions `extract_existence`, `extract_numeric` and `collect_features` (FeatureExtraction.ipynb)\n",
    "3. Task: Write the function `transform_grammar` (TransformGrammar.ipynb)\n",
    "\n",
    "**Activity 2:** Train Classification Model\n",
    "\n",
    "4. Task: Write a function `train_tree` (DecisionTreeLearner.ipynb)\n",
    "\n",
    "**Activity 5:** Generate new Inputs Files\n",
    "\n",
    "5. Task: Write a function `generate_samples` (GenerateSamples.ipynb)\n",
    "\n",
    "\n",
    "(Please refer to the individual notebooks for a detailed description of the individual tasks.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae7a2b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Paper: Read the paper of Kampmann et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a155f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-success alertsuccess\">\n",
    "[Task] Before you continure, you should take a first look at intial <a href=\"https://publications.cispa.saarland/3107/7/fse2020-alhazen.pdf\">paper</a> to understand the proposed approach and underlying concepts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960bad47",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02cbcb6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To illustrate the use-case and the necessity of _Alhazen_, we start with a quick motivating example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64868444",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Motivating Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01cbf4a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To test _Alhazen_, we implemented the simple CALCULATOR example from the paper. We furthermore implemented a synthetic `BUG` that you have to explain utilizing the decision tree learned by _Alhazen_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f77e437",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "To make the bug explanation a little bit more challenging, we altered to calculator behavior. The introduced bug of Kampmann et al. is not the same as ours.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6cad4c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The calculator takes an input file as input and returns whether the bug was present or not (`BUG,` `NO_BUG`). Input files for the calculator have to conform to the `CALCULATOR`- Grammar. Let us have a look at the grammar definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12438a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load the grammar from the calculator\n",
    "from alhazen_formalizations.calculator import grammar_alhazen as grammar\n",
    "\n",
    "for rule in grammar:\n",
    "    print(rule.ljust(20), grammar[rule])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce13a31",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that the `CALCULATOR` Grammar consists of several production rules. The calculator subject will only accept inputs that conform to this grammar definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25aa175",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "[Info]: We use the functionallity provided by <a href=\"https://www.fuzzingbook.org\">The Fuzzingbook</a>. For a more detailed description of Grammars, have a look at the chapter <a href=\"https://www.fuzzingbook.org/html/Grammars.html\">Fuzzing with Grammars</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e18bce",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, lets load the initial input samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alhazen_formalizations.calculator import initial_inputs\n",
    "\n",
    "display(initial_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4079afa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The two initial imput samples for our calculator should be:\n",
    "- _cos(12)_\n",
    "- _sqrt(-900)_\n",
    "\n",
    "Lets check if this is true with python's `assert` function. The condition is True, if no Assertion is thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abadba57",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First sample\n",
    "assert initial_inputs[0] == 'cos(12)', f\"The loaded sample {sample_list[0]} and cos(12) are not equal.\"\n",
    "\n",
    "# Second sample\n",
    "assert initial_inputs[1] == 'sqrt(-900)', f\"The loaded sample {sample_list[1]} and sqrt(-900) are not equal.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c945e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We intentionally throw an assertion error. sample_list[0] is equal to sqrt(-16)\n",
    "try:\n",
    "    assert initial_inputs[0] == 'cos(4)', f\"The loaded sample {sample_list[0]} and cos(4) are not equal.\"\n",
    "except AssertionError as e: \n",
    "    print(\"Expected Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9529792a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's execute our two input samples and observe the calculator's behavior. To do this, we load the function `execute_samples` from the notebook ExecuteSamples.ipynb. We can call the function with a list of input samples, and it returns the corresponding execution outcome (label/oracle). The output is a [pandas dataframe](https://pandas.pydata.org/docs/reference/frame.html), and the labels are from the class `OracleResults`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d43b7b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load function execute samples\n",
    "from ipynb.fs.defs.Activity5_ExecuteSamples import execute_samples\n",
    "\n",
    "# execute_samples(List[str])\n",
    "oracle = execute_samples(initial_inputs)\n",
    "if display_output: display(oracle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19d09b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "[Info]: For a more detailed description of the functionallity, you can look into the implementation of <i>execute_samples</i> in the \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58c5b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combined sample and labels by iterating over the obtained oracle\n",
    "if display_output: \n",
    "    for i, row in enumerate(oracle['oracle']): print(sample_list[i].ljust(30) + str(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6bcb4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We observe that the sample `sqrt(-16)` triggers a bug in the calculator, whereas the sample `sqrt(4)` does not show unusual behavior. Of course, we want to know why the sample fails the program. In a typical use case, the developers of the calculator program would now try other input samples and evaluate if similar inputs also trigger the program's failure. Let's try some more input samples; maybe we can refine our understanding of why the calculator crashes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c725c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Our guesses (maybe the failure is also in the cos or tan function?)\n",
    "guess_samples = ['cos(-16)', 'tan(-16)', 'sqrt(-100)', 'sqrt(-20.23412431234123)']\n",
    "\n",
    "# lets obtain the execution outcome for each of our guess\n",
    "guess_oracle = execute_samples(guess_samples)\n",
    "\n",
    "# lets show the results\n",
    "if display_output: \n",
    "    for i, row in enumerate(guess_oracle['oracle']): print(guess_samples[i].ljust(30) + str(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a143520",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It looks like the failure only occurs in the `sqrt` function, however, only for specific `x` values. We could now try other values for `x` and repeat the process. However, this would be highly time-consuming and not an efficient debugging technique for a larger and more complex test subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a43ace3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wouldn't it be great if there was a tool that automatically does this for us? And this is exactly what _Alhazen_ is used for. It helps us explain why specific input files fail a program. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3337f240",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "[Info]: <i>Alhazen</i> is a tool that automatically learns the circumstances of program failure by associating syntactical features of sample inputs with the execution outcome. The produced explanations (in the form of a decision tree) help developers focus on the input space's relevant aspects.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c861b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Implementing Alhazen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249e11e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<hr/>\n",
    "<div class=\"alert alert-success alertsuccess\">\n",
    "[Task]: Complete the missing functions of <i>Alhazen</i> and explain what input samples result in the calculator's failure.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0acf7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To help you complete Alhazen, we have already provided you with a complete framework for feedback loop. Your goal is to implement the missing functions such that _Alhazen_ can learn the circumstances of program failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc99eba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You are required to implement the following functions: `extract_existence`, `extract_numeric`, `collect_features`, `transform_grammar`, `train_tree`, and `generate_samples`.\n",
    "\n",
    "Please follow the instructions in the individual notebooks for a more detailed documentation of the required functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b74ff7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following code cell imports all functions from the other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be426322",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "from ipynb.fs.full.Activity1_1_FeatureExtraction import extract_existence, extract_numeric, collect_features\n",
    "\n",
    "# transfrom grammar\n",
    "from ipynb.fs.full.Activity1_2_GrammarTransformation import transform_grammar\n",
    "\n",
    "# learn decision tree \n",
    "from ipynb.fs.full.Activity2_DecisionTreeLearner import train_tree\n",
    "\n",
    "# generate new input files\n",
    "# from ipynb.fs.full.Activity4_GenerateSamples import generate_samples\n",
    "from ipynb.fs.full.Activity4_GenerateSamples import generate_samples_advanced\n",
    "from ipynb.fs.full.Activity4_GenerateSamples import generate_samples_random as generate_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f543a2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "Additionally to the feedback loop, we provide you with an implementation of the `execute_sample` and the `get_all_input_specifications` functions. Please look at the corresponding notebooks for a detailed description of how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9763666",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# extract features from the decision tree (provided by us)\n",
    "from ipynb.fs.full.Activity3_RequirementExtraction import get_all_input_specifications\n",
    "\n",
    "# execute samples (provided by us)\n",
    "from ipynb.fs.defs.Activity5_ExecuteSamples import execute_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029c1db",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##  Alhazen Class\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607bb8dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas\n",
    "\n",
    "from fuzzingbook.Grammars import Grammar\n",
    "from ipynb.fs.full.helper import OracleResult\n",
    "from ipynb.fs.full.helper import show_tree\n",
    "\n",
    "GENERATOR_TIMEOUT = 10 # timeout in seconds\n",
    "\n",
    "class Alhazen:\n",
    "    \n",
    "    def __init__(self, initial_inputs: List[str],\n",
    "                 grammar: Grammar,\n",
    "                 max_iter: int = 10,\n",
    "                 generator_timeout: int = 10):\n",
    "        \n",
    "        self._initial_inputs = initial_inputs\n",
    "        self._grammar = grammar\n",
    "        self._max_iter = max_iter\n",
    "        self._previous_samples = None\n",
    "        self._data = None\n",
    "        self._trees = []\n",
    "        self._generator_timeout = generator_timeout\n",
    "        self._setup()\n",
    "        \n",
    "    def _setup(self):\n",
    "        self._previous_samples = self._initial_inputs\n",
    "        \n",
    "        self._all_features = extract_existence(self._grammar) + extract_numeric(self._grammar)\n",
    "        self._feature_names = [f.name for f in self._all_features]\n",
    "        \n",
    "    def run(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _add_new_data(self, exec_data, feature_data):\n",
    "        joined_data = exec_data.join(feature_data.drop(['sample'], axis=1))\n",
    "        \n",
    "        # Only add valid data\n",
    "        new_data = joined_data[(joined_data['oracle'] != OracleResult.UNDEF)]\n",
    "        new_data = joined_data.drop(joined_data[joined_data.oracle.astype(str) == \"UNDEF\"].index)\n",
    "        if 0 != len(new_data):\n",
    "            if self._data is None:\n",
    "                self._data = new_data\n",
    "            else:\n",
    "                self._data = pandas.concat([self._data, new_data], sort=False)\n",
    "                \n",
    "    def _finalize(self):\n",
    "        return self._trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ff7cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Alhazen(Alhazen):\n",
    "    \n",
    "    def run(self):\n",
    "        for iteration in range(self._max_iter):\n",
    "            print(f\"Starting Iteration: \" + str(iteration))\n",
    "            self._loop(self._previous_samples)\n",
    "            \n",
    "        return self._finalize()\n",
    "        \n",
    "    \n",
    "    def _loop(self, sample_list):\n",
    "        # obtain labels, execute samples (Initial Step, Activity 5)\n",
    "        exec_data = execute_samples(sample_list)\n",
    "        \n",
    "        # collect features from the new samples (Activity 1)\n",
    "        feature_data = collect_features(sample_list, self._grammar)\n",
    "        \n",
    "        # combine the new data with the already existing data\n",
    "        self._add_new_data(exec_data, feature_data)\n",
    "        \n",
    "        # train a tree (Activity 2)\n",
    "        dec_tree = train_tree(self._data)\n",
    "        self._trees.append(dec_tree)\n",
    "        \n",
    "        # extract new requirements from the tree (Activity 3)\n",
    "        new_input_specifications = get_all_input_specifications(dec_tree, \n",
    "                                                self._all_features, \n",
    "                                                self._feature_names, \n",
    "                                                self._data.drop(['oracle'], axis=1))\n",
    "        \n",
    "        # generate new inputs according to the new input specifications\n",
    "        # (Activity 4)\n",
    "        new_samples = generate_samples(self._grammar, new_input_specifications, self._generator_timeout)\n",
    "        self._previous_samples = new_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e8130",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "</hr>\n",
    "\n",
    "If you have correctly implemented the missing functions, we can finally run _Alhazen_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c045b9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzingbook.Grammars import Grammar, is_valid_grammar\n",
    "import string\n",
    "\n",
    "HEARTBLEED: Grammar = {\n",
    "    \"<start>\": [\"<length>\" \" x\" \"<payload>\" \" y<padding>\"],\n",
    "    \"<length>\": [\"<number>\"],\n",
    "    \"<number>\": [\"<onenine><maybe_digits>\"],\n",
    "    \"<onenine>\": [str(num) for num in range(1, 10)],\n",
    "    \"<maybe_digits>\": [\"\", \"<digits>\"],\n",
    "    \"<digits>\": [\"<digit>\", \"<digit><digits>\"],\n",
    "    \"<digit>\": list(string.digits),\n",
    "\n",
    "    \"<payload>\": [\"\", \"<digit><payload>\"],\n",
    "    \"<padding>\": [\"\", \"<digit><padding>\"]\n",
    "}\n",
    "\n",
    "assert is_valid_grammar(HEARTBLEED)\n",
    "\n",
    "positive_inputs = [\"6 x3 y13\", \"125 x4 y\", \"6512 x10 y2\", \"7992 x66 y39337874\"]\n",
    "negative_inputs = [\"4 x875611395 y3\", \"12 x123456789101 y3\"]\n",
    "\n",
    "sample_list = positive_inputs + negative_inputs\n",
    "sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bb59b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set the number of refinement iterations and the timeout for the input generator\n",
    "# the execution time of Alhazen mainly depends on the number of iterations\n",
    "\n",
    "GRAMMAR = HEARTBLEED\n",
    "MAX_ITERATION = 20\n",
    "GENERATOR_TIMEOUT = 10 # timeout in seconds\n",
    "\n",
    "# let's initialize Alhazen\n",
    "# let's use the previously used sample_list (['sqrt(-16)', 'sqrt(4)'])\n",
    "alhazen = Alhazen(sample_list, GRAMMAR, MAX_ITERATION, GENERATOR_TIMEOUT)\n",
    "\n",
    "# and run it\n",
    "# Alhazen returns a list of all the iteratively learned decision trees\n",
    "trees = alhazen.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dac150",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "</hr>\n",
    "\n",
    "Let's display the final decision tree learned by Alhazen. You can use the function `show_tree(decison_tree, features)` to display the final tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf8558",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.requirementExtractionDT.treetools import remove_unequal_decisions\n",
    "\n",
    "all_features = extract_existence(GRAMMAR) + extract_numeric(GRAMMAR)\n",
    "# show_tree(trees[MAX_ITERATION-1], all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68856cd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "[Info] The decision tree may contain unneccesary long paths, where the bug-class does not change. You can use the function 'remove_unequal_decisions(decision_tree)' to remove those nodes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270c727",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.requirementExtractionDT.treetools import remove_unequal_decisions\n",
    "\n",
    "show_tree(remove_unequal_decisions(trees[MAX_ITERATION-1]), all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551eef68",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should now be able to identify the features that are responsible for the caluclator's failue!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a805fe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`Real Solution`: The failure occurs whenever the function 'sqrt(x)' is used and x is between '-12' and '-42'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed8482",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01e4b07",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<hr/>\n",
    "Let's evaluate the learned classification model! We judge the quality of the learned decision tree learner by assessing its capabilities of predicting the behavior of newly generated inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b01c9e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation Setup (Generating an Evaluation Data Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d42316",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the first step of evaluation of the learned classifier, we generate a evaluation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d6774c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We import the GrammarFuzzer\n",
    "from fuzzingbook.GrammarFuzzer import GrammarFuzzer\n",
    "\n",
    "evaluation_data = []\n",
    "\n",
    "# And generate 1000 input samples\n",
    "g = GrammarFuzzer(GRAMMAR)\n",
    "for i in range(1000):\n",
    "    evaluation_data.append(str(g.fuzz()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93000c99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Lets obtain the actuall program behavior of the evaluation data ['BUG', 'NO_BUG']\n",
    "evaluation_exec_data = execute_samples(evaluation_data)\n",
    "print(evaluation_exec_data) \n",
    "\n",
    "# Is the data set imbalanced? \n",
    "sample_bug_count = len(evaluation_exec_data[(evaluation_exec_data[\"oracle\"].astype(str) == \"BUG\")])\n",
    "sample_count = len(evaluation_exec_data)\n",
    "\n",
    "print(f\"{sample_bug_count} samples of {sample_count} generated inputs trigger the bug.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccfc5ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# let us obtain the features from the generated inputs\n",
    "eval_feature_data = collect_features(evaluation_data, GRAMMAR)\n",
    "# display(eval_feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d07903",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up the evaluation data\n",
    "joined_data = evaluation_exec_data.join(eval_feature_data.drop(['sample'], axis=1))\n",
    "\n",
    "# Only add valid data\n",
    "new_data = joined_data[(joined_data['oracle'] != OracleResult.UNDEF)]\n",
    "clean_data = joined_data.drop(joined_data[joined_data.oracle.astype(str) == \"UNDEF\"].index).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db58ac4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation Results\n",
    "\n",
    "<hr/>\n",
    "Let's use the generated evaluation set to measure the accuracy, precision, recall and f1-score of your learned machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6852e0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "[Info] We use <a href=\"https://scikit-learn.org/stable/\">scikit-learn</a> to evalute the classifier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05212fe9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_iteration = MAX_ITERATION - 1\n",
    "final_tree = remove_unequal_decisions(trees[eval_iteration])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ccd4a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We use the final decision tree to predict the behavior of the evaluation data set.\n",
    "predictions = final_tree.predict(clean_data.drop(['oracle'], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52430546",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's measure the accuracy of the learned decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd97ae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "[Info] We start by measuering the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\">accuracy</a> of the classifier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50054bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We calculate the accuracy by comparing how many predictions match the actual program behavior\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(clean_data['oracle'].astype(str), predictions, normalize=True)\n",
    "# we round the accuracy to three digits\n",
    "accuracy = round(accuracy*100, 3)\n",
    "print(f\"The decison tree at iteration {str(eval_iteration)} achieved an accuracy of {accuracy} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7c0d5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "[Info] We use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\">precision-score</a> and the <a href=\"https://scikit-learn.org/stable/\">recall-score</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033c58e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "precision = precision_score(clean_data['oracle'].astype(str), predictions, pos_label='BUG', average='binary')\n",
    "precision = round(precision*100, 3)\n",
    "recall = recall_score(clean_data['oracle'].astype(str), predictions, pos_label='BUG', average='binary')\n",
    "recall = round(recall*100, 3)\n",
    "\n",
    "print(f\"The decison tree at iteration {str(eval_iteration)} achieved a precision of {precision} %\")\n",
    "print(f\"The decison tree at iteration {str(eval_iteration)} achieved a recall of {recall} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4e549",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "[Info] To counteract the imbalanced data set, we use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\">F1-score</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a13c19",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(clean_data['oracle'].astype(str), predictions, pos_label='BUG', average='binary')\n",
    "print(f\"The decison tree at iteration {str(eval_iteration)} achieved a f1-score of {round(f1, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8764dc1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Congratulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631aeca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You did it, congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa94a89",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
